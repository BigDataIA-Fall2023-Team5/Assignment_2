{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange; font-weight:bold\">Note: To answer questions based on text documents, we recommend the procedure in <a href=\"https://github.com/openai/openai-cookbook/blob/main/examples/Question_answering_using_embeddings.ipynb\">Question Answering using Embeddings</a>. Some of the code below may rely on <a href=\"https://github.com/openai/openai-cookbook/tree/main/transition_guides_for_deprecated_API_endpoints\">deprecated API endpoints</a>.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train a fine-tuning model specialized for Q&A\n",
    "This notebook will utilize the dataset of context, question and answer pairs to additionally create adversarial questions and context pairs, where the question was not generated on that context. In those cases the model will be prompted to answer \"No sufficient context for answering the question\". We will also train a discriminator model, which predicts whether the question can be answered based on the context or not.\n",
    "\n",
    "We will add hard adversarial examples as well, which will be based either on semantically similar sections, or neighbouring sections, originating from the same article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in ./.conda/lib/python3.8/site-packages (0.28.1)\n",
      "Requirement already satisfied: requests>=2.20 in ./.conda/lib/python3.8/site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: tqdm in ./.conda/lib/python3.8/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in ./.conda/lib/python3.8/site-packages (from openai) (3.8.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/lib/python3.8/site-packages (from requests>=2.20->openai) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.8/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.conda/lib/python3.8/site-packages (from requests>=2.20->openai) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.8/site-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.conda/lib/python3.8/site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./.conda/lib/python3.8/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./.conda/lib/python3.8/site-packages (from aiohttp->openai) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./.conda/lib/python3.8/site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./.conda/lib/python3.8/site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./.conda/lib/python3.8/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.conda/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: scikit-learn in ./.conda/lib/python3.8/site-packages (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.conda/lib/python3.8/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.conda/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in ./.conda/lib/python3.8/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in ./.conda/lib/python3.8/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.conda/lib/python3.8/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.conda/lib/python3.8/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number</th>\n",
       "      <th>Description</th>\n",
       "      <th>Last_Updated</th>\n",
       "      <th>SEC_Number</th>\n",
       "      <th>Topic</th>\n",
       "      <th>PDF_Link</th>\n",
       "      <th>pyPDF_extraction</th>\n",
       "      <th>tokens</th>\n",
       "      <th>questions</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Application for registration or exemption from...</td>\n",
       "      <td>Feb. 1999</td>\n",
       "      <td>SEC1935</td>\n",
       "      <td>Self-Regulatory Organizations</td>\n",
       "      <td>https://www.sec.gov/files/form1-e.pdf</td>\n",
       "      <td>You may not send a completed printout of this ...</td>\n",
       "      <td>1581</td>\n",
       "      <td>1) What is the purpose of Form 1-E mentioned i...</td>\n",
       "      <td>1.The purpose of Form 1-E mentioned in the tex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-A</td>\n",
       "      <td>Regulation A Offering Statement (PDF)</td>\n",
       "      <td>Sept. 2021</td>\n",
       "      <td>SEC486</td>\n",
       "      <td>Securities Act of 1933, Small Businesses</td>\n",
       "      <td>https://www.sec.gov/files/form1-k.pdf</td>\n",
       "      <td>UNITED STATES SECURITIES AND EXCHANGE COMMISSI...</td>\n",
       "      <td>2000</td>\n",
       "      <td>1. What is the purpose of Form 1-K?\\n2. What a...</td>\n",
       "      <td>1.The purpose of Form 1-K is to provide an ann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-E</td>\n",
       "      <td>Notification under Regulation E (PDF)</td>\n",
       "      <td>Aug. 2001</td>\n",
       "      <td>SEC1807</td>\n",
       "      <td>Investment Company Act of 1940, Small Business...</td>\n",
       "      <td>https://www.sec.gov/files/form1-n.pdf</td>\n",
       "      <td>OMB APPROVAL OMB Number 3235 0554 Expires Febr...</td>\n",
       "      <td>2000</td>\n",
       "      <td>1. What is the purpose of Form 1-N?\\n\\n2. How ...</td>\n",
       "      <td>1.The purpose of Form 1-N is to serve as a not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-K</td>\n",
       "      <td>Annual Reports and Special Financial Reports (...</td>\n",
       "      <td>Sept. 2021</td>\n",
       "      <td>SEC2913</td>\n",
       "      <td>Securities Act of 1933, Small Businesses</td>\n",
       "      <td>https://www.sec.gov/files/form1-sa.pdf</td>\n",
       "      <td>UNITED STATES SECURITIES AND EXCHANGE COMMISSI...</td>\n",
       "      <td>2000</td>\n",
       "      <td>1. What is the purpose of Form 1 SA?\\n2. How o...</td>\n",
       "      <td>1.The purpose of Form 1 SA is to file semiannu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-N</td>\n",
       "      <td>Form and amendments for notice of registration...</td>\n",
       "      <td>Dec. 2013</td>\n",
       "      <td>SEC2568</td>\n",
       "      <td>Securities Exchange Act of 1934, Self-Regulato...</td>\n",
       "      <td>https://www.sec.gov/files/form1-u.pdf</td>\n",
       "      <td>OMB APPROVAL OMB Number 3235 0722 Expires Dece...</td>\n",
       "      <td>2000</td>\n",
       "      <td>1. What is the purpose of Form 1-U?\\n\\n2. What...</td>\n",
       "      <td>1.The purpose of Form 1-U is to file a current...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Number                                        Description Last_Updated  \\\n",
       "0      1  Application for registration or exemption from...    Feb. 1999   \n",
       "1    1-A              Regulation A Offering Statement (PDF)   Sept. 2021   \n",
       "2    1-E              Notification under Regulation E (PDF)    Aug. 2001   \n",
       "3    1-K  Annual Reports and Special Financial Reports (...   Sept. 2021   \n",
       "4    1-N  Form and amendments for notice of registration...    Dec. 2013   \n",
       "\n",
       "  SEC_Number                                              Topic  \\\n",
       "0    SEC1935                      Self-Regulatory Organizations   \n",
       "1     SEC486           Securities Act of 1933, Small Businesses   \n",
       "2    SEC1807  Investment Company Act of 1940, Small Business...   \n",
       "3    SEC2913           Securities Act of 1933, Small Businesses   \n",
       "4    SEC2568  Securities Exchange Act of 1934, Self-Regulato...   \n",
       "\n",
       "                                 PDF_Link  \\\n",
       "0   https://www.sec.gov/files/form1-e.pdf   \n",
       "1   https://www.sec.gov/files/form1-k.pdf   \n",
       "2   https://www.sec.gov/files/form1-n.pdf   \n",
       "3  https://www.sec.gov/files/form1-sa.pdf   \n",
       "4   https://www.sec.gov/files/form1-u.pdf   \n",
       "\n",
       "                                    pyPDF_extraction  tokens  \\\n",
       "0  You may not send a completed printout of this ...    1581   \n",
       "1  UNITED STATES SECURITIES AND EXCHANGE COMMISSI...    2000   \n",
       "2  OMB APPROVAL OMB Number 3235 0554 Expires Febr...    2000   \n",
       "3  UNITED STATES SECURITIES AND EXCHANGE COMMISSI...    2000   \n",
       "4  OMB APPROVAL OMB Number 3235 0722 Expires Dece...    2000   \n",
       "\n",
       "                                           questions  \\\n",
       "0  1) What is the purpose of Form 1-E mentioned i...   \n",
       "1  1. What is the purpose of Form 1-K?\\n2. What a...   \n",
       "2  1. What is the purpose of Form 1-N?\\n\\n2. How ...   \n",
       "3  1. What is the purpose of Form 1 SA?\\n2. How o...   \n",
       "4  1. What is the purpose of Form 1-U?\\n\\n2. What...   \n",
       "\n",
       "                                             answers  \n",
       "0  1.The purpose of Form 1-E mentioned in the tex...  \n",
       "1  1.The purpose of Form 1-K is to provide an ann...  \n",
       "2  1.The purpose of Form 1-N is to serve as a not...  \n",
       "3  1.The purpose of Form 1 SA is to file semiannu...  \n",
       "4  1.The purpose of Form 1-U is to file a current...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "df = pd.read_csv('/Users/anamikabharali/Documents/UNI/BD/Assignment2/form_qa.csv')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the sections into a training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df1, test_df1 = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_midpoint = len(train_df1)//4\n",
    "test_midpoint = len(test_df1)//4\n",
    "train_df = train_df1.iloc[:train_midpoint]\n",
    "test_df = test_df1.iloc[:test_midpoint]\n",
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we check that the separator we intend to use isn't present within the contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.pyPDF_extraction.str.contains('->').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Create the fine-tuning datasets for Q&A and discriminator models\n",
    "The fine-tuning dataset is created in the following way. For every corresponding question, answer and context pair we create:\n",
    "- Positive example: correct question, answer, context pair\n",
    "- Negative examples:\n",
    "  - random negative example, where the random context is paired with the question \n",
    "  - two hard negative examples\n",
    "    - one originating from the same wikipedia article\n",
    "    - another, which is most similar to the correct context\n",
    "\n",
    "This process is noisy, as sometimes the question might be answerable given a different context, but on average we hope this won't affect the peformance too much.\n",
    "\n",
    "We apply the same process of dataset creation for both the discriminator, and the Q&A answering model. We apply the process separately for the training and testing set, to ensure that the examples from the training set don't feature within the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['answers'] = df['answers'].astype('string')\n",
    "\n",
    "df['questions'] = df['questions'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "string[python]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes['answers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    }
   ],
   "source": [
    "df_splitpoint = len(df)//2\n",
    "df = df.iloc[:df_splitpoint]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_random_similar_contexts(question, pyPDF_extraction, search_model='ada', max_rerank=10):\n",
    "    \"\"\"\n",
    "    Find similar contexts to the given context using the search file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        results = openai.Engine(search_model).search(\n",
    "            search_model=search_model, \n",
    "            query=question, \n",
    "            max_rerank=max_rerank,\n",
    "            #file=file_id\n",
    "        )\n",
    "        candidates = []\n",
    "        for result in results['data'][:3]:\n",
    "            if result['text'] == pyPDF_extraction:\n",
    "                continue\n",
    "            candidates.append(result['text'])\n",
    "        random_candidate = random.choice(candidates)\n",
    "        return random_candidate\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"\"\n",
    "\n",
    "def create_fine_tuning_dataset(df, discriminator=False, n_negative=1, add_related=False):\n",
    "    \"\"\"\n",
    "    Create a dataset for fine tuning the OpenAI model; either for a discriminator model, \n",
    "    or a model specializing in Q&A, where it says if no relevant context is found.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        The dataframe containing the question, answer and context pairs\n",
    "    discriminator: bool\n",
    "        Whether to create a dataset for the discriminator\n",
    "    n_negative: int\n",
    "        The number of random negative samples to add (using a random context)\n",
    "    add_related: bool\n",
    "        Whether to add the related contexts to the correct context. These are hard negative examples\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The dataframe containing the prompts and completions, ready for fine-tuning\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for i, row in df.iterrows():\n",
    "        for q, a in zip((str(\"1.\") + str(row['questions'])).split('\\n'), (str(\"1.\") + str(row['answers'])).split('\\n')):\n",
    "            #(\"1.\" + row.questions).split('\\n'), (\"1.\" + row.answers).split('\\n')\n",
    "            if len(q) >10 and len(a) >10:\n",
    "                if discriminator:\n",
    "                    rows.append({\"prompt\":f\"{row.pyPDF_extraction}\\nQuestion: {q[2:].strip()}\\n Related:\", \"completion\":f\" yes\"})\n",
    "                else:\n",
    "                    rows.append({\"prompt\":f\"{row.pyPDF_extraction}\\nQuestion: {q[2:].strip()}\\nAnswer:\", \"completion\":f\" {a[2:].strip()}\"})\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        for q in (str(\"1.\") + str(row['questions'])).split('\\n'):\n",
    "            if len(q) >10:\n",
    "                for j in range(n_negative + (2 if add_related else 0)):\n",
    "                    random_context = \"\"\n",
    "                    if j == 0 and add_related:\n",
    "                        # add the related contexts based on originating from the same wikipedia page\n",
    "                        subset = df[(df.Description == row.Description) & (df.pyPDF_extraction != row.pyPDF_extraction)]\n",
    "                        \n",
    "                        if len(subset) < 1:\n",
    "                            continue\n",
    "                        random_context = subset.sample(1).iloc[0].pyPDF_extraction\n",
    "                    if j == 1 and add_related:\n",
    "                        # add the related contexts based on the most similar contexts according to the search\n",
    "                        random_context = get_random_similar_contexts(q[2:].strip(), row.pyPDF_extraction, search_model='ada', max_rerank=10)\n",
    "                    else:\n",
    "                        while True:\n",
    "                            # add random context, which isn't the correct context\n",
    "                            random_context = df.sample(1).iloc[0].pyPDF_extraction\n",
    "                            if random_context != row.pyPDF_extraction:\n",
    "                                break\n",
    "                    if discriminator:\n",
    "                        rows.append({\"prompt\":f\"{random_context}\\nQuestion: {q[2:].strip()}\\n Related:\", \"completion\":f\" no\"})\n",
    "                    else:\n",
    "                        rows.append({\"prompt\":f\"{random_context}\\nQuestion: {q[2:].strip()}\\nAnswer:\", \"completion\":f\" No appropriate context found to answer the question.\"})\n",
    "\n",
    "    return pd.DataFrame(rows) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply the same process of dataset creation for both the discriminator, and the Q&A answering model. We apply the process separately for the training and testing set, to ensure that the examples from the training set don't feature within the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n",
      "search\n"
     ]
    }
   ],
   "source": [
    "for name, is_disc in [('discriminator', True), ('qa', False)]:\n",
    "    for train_test, dt in [('train', train_df), ('test', test_df)]:\n",
    "        ft = create_fine_tuning_dataset(dt, discriminator=is_disc, n_negative=1, add_related=True)\n",
    "        ft.to_json(f'{name}_{train_test}.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We formatted the data according to the recommendations from the fine-tuning tool, which is available using\n",
    "> openai tools fine_tunes.prepare_data -f qa_train.jsonl\n",
    "\n",
    "We highly recommend that you use this tool, which suggests improvements in your data formatting for fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Submit the datasets for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['OPENAI_API_KEY'] = getpass.getpass() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|████████████████████| 1.41M/1.41M [00:00<00:00, 1.22Git/s]\n",
      "Uploaded file from discriminator_train.jsonl: file-0SbB8QuQ3iKuS0jZhWjh5htz\n",
      "Upload progress: 100%|███████████████████████| 380k/380k [00:00<00:00, 485Mit/s]\n",
      "Uploaded file from discriminator_test.jsonl: file-wcQc2xgpEimwabSvcQrkw8OE\n",
      "Created fine-tune: ft-gM3QaNAKHAv3DIQlFw8D6zQT\n",
      "Streaming events until fine-tuning is complete...\n",
      "\n",
      "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
      "[2023-10-20 10:41:16] Created fine-tune: ft-gM3QaNAKHAv3DIQlFw8D6zQT\n",
      "[2023-10-20 10:41:34] Fine-tune costs $0.44\n",
      "[2023-10-20 10:41:34] Fine-tune enqueued. Queue number: 0\n",
      "[2023-10-20 10:41:36] Fine-tune started\n",
      "[2023-10-20 10:42:14] Completed epoch 1/4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!openai api fine_tunes.create -t \"discriminator_train.jsonl\" -v \"discriminator_test.jsonl\" --batch_size 16  --compute_classification_metrics --classification_positive_class \" yes\" --model ada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload progress: 100%|████████████████████| 1.46M/1.46M [00:00<00:00, 1.31Git/s]\n",
      "Uploaded file from qa_train.jsonl: file-hH5LyxOFIvHHCpx8E0HBMTNy\n",
      "Upload progress: 100%|███████████████████████| 352k/352k [00:00<00:00, 450Mit/s]\n",
      "Uploaded file from qa_test.jsonl: file-6fxtQGjmq5s1JxAn6NcYVjAJ\n",
      "Created fine-tune: ft-4ObGIC1sednrv2oThOo6dCWr\n",
      "Streaming events until fine-tuning is complete...\n",
      "\n",
      "(Ctrl-C will interrupt the stream, but not cancel the fine-tune)\n",
      "[2023-10-20 10:57:06] Created fine-tune: ft-4ObGIC1sednrv2oThOo6dCWr\n",
      "[2023-10-20 10:57:22] Fine-tune costs $3.43\n",
      "[2023-10-20 10:57:22] Fine-tune enqueued. Queue number: 0\n",
      "[2023-10-20 10:57:23] Fine-tune started\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!openai api fine_tunes.create -t \"qa_train.jsonl\" -v \"qa_test.jsonl\" --batch_size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Using the fine-tuned models\n",
    "\n",
    "We will now use the fine-tuned discriminator and the fine-tuned Q&A model. By requesting logprobs, we can see how certain the discriminator is in a `yes` vs `no` answer.\n",
    "\n",
    "The fine tune models created above can be checked by writing the following comands in the terminal \n",
    "curl https://api.openai.com/v1/fine-tunes/ft-4ObGIC1sednrv2oThOo6dCWr   \\                                                                   \n",
    "  -H \"Authorization: Bearer $OPENAI_API_KEY\"\n",
    "\n",
    "Where ft-4ObGIC1sednrv2oThOo6dCWr is the created fine-tune id and $OPENAI_API_KEY is your openAI key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<OpenAIObject at 0x7fc9d0dd92c0> JSON: {\n",
       "   \" yes\": -4.52315,\n",
       "   \" no\": -0.011236884\n",
       " }]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_discriminator = \"ada:ft-personal-2023-10-20-14-43-29\"\n",
    "ft_qa = \"curie:ft-personal-2023-10-20-14-59-56\"\n",
    "\n",
    "def apply_ft_discriminator(context, question, discriminator_model):\n",
    "    \"\"\"\n",
    "    Apply the fine tuned discriminator to a question, to assess whether it can be answered from the context.\n",
    "    \"\"\"\n",
    "    prompt = f\"{context}\\nQuestion: {question}\\n Related:\"\n",
    "    result = openai.Completion.create(model=discriminator_model, prompt=prompt, max_tokens=1, temperature=0, top_p=1, n=1, logprobs=2)\n",
    "    return result['choices'][0]['logprobs']['top_logprobs']\n",
    "\n",
    "apply_ft_discriminator('The first human-made object in space was the Soviet Union satellite Sputnik 1 on 4 October 1957.', \n",
    "                        'What was the first human-made object in space?', ft_discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model can generalize well to different contexts and questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The first human-made object in space was the Soviet Union satellite Sputnik 1 on 4 October 1957'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_ft_qa_answer(context, question, answering_model):\n",
    "    \"\"\"\n",
    "    Apply the fine tuned discriminator to a question\n",
    "    \"\"\"\n",
    "    prompt = f\"{context}\\nQuestion: {question}\\nAnswer:\"\n",
    "    result = openai.Completion.create(model=answering_model, prompt=prompt, max_tokens=30, temperature=0, top_p=1, n=1, stop=['.','\\n'])\n",
    "    return result['choices'][0]['text']\n",
    "\n",
    "apply_ft_qa_answer('The first human-made object in space was the Soviet Union satellite Sputnik 1 on 4 October 1957.', \n",
    "                    'What was the first human-made object in space?', ft_qa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model can answer the question, when the context is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The Soviet Union was the first country to launch a satellite into space'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_ft_qa_answer('The first human-made object in space was the Soviet Union satellite Sputnik 1 on 4 October 1957.',\n",
    "                    'What is impressive about the Soviet Union?', ft_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' No appropriate context found to answer the question'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_ft_qa_answer('The first human-made object in space was the Soviet Union satellite Sputnik 1 on 4 October 1957.',\n",
    "                    'How many cars were produced in the Soviet Union in 1970?', ft_qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model knows when to answer the question, and when to say that insufficient context is present to answer the question."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit ('3.9.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cb9817b186a29e4e9713184d901f26c1ee05ad25243d878baff7f31bb1fef480"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
